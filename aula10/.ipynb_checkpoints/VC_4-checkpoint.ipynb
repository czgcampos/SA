{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, time\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# fixar random seed para se puder reproduzir os resultados\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 1 - preparar o dataset\n",
    "'''\n",
    "fazer o download da sequencias do valor das ações da google GOOGL stock data (fonte yahoo.com)\n",
    "dataset: http://chart.finance.yahoo.com/table.csv?s=GOOGL&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv\n",
    "A função get_stock_data é generica para ir buscar dados à yahoo.com\n",
    "trata-se uma tabela com: ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "Vamos śo utilizar os campos ['Open','High','Close']\n",
    "'''\n",
    "def get_stock_data(stock_name, normalized=0,file_name=None):\n",
    "    if not file_name:\n",
    "        file_name = 'http://chart.finance.yahoo.com/table.csv?s=%s&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv' % stock_name\n",
    "    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "    stocks = pd.read_csv(file_name, header=0, names=col_names) #fica numa especie de tabela exactamente como estava no csv (1350 linhas,7 colunas)\n",
    "    df = pd.DataFrame(stocks) #neste caso não vai fazer nada\n",
    "    date_split = df['Date'].str.split('-').str #não vai servir para nada\n",
    "    df['Year'], df['Month'], df['Day'] = date_split #não vai servir para nada\n",
    "    df[\"Volume\"] = df[\"Volume\"] / 10000 #não vai servir para nada\n",
    "    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) #vou só ficar com as colunas 1,2,4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GOOGL_stock_dataset():\n",
    "    stock_name = 'GOOGL'\n",
    "    return get_stock_data(stock_name,0, 'table.csv')\n",
    "\n",
    "def pre_processar_GOOGL_stock_dataset(df):\n",
    "    df['High'] = df['High'] / 100\n",
    "    df['Open'] = df['Open'] / 100\n",
    "    df['Close'] = df['Close'] / 100\n",
    "    return df\n",
    "\n",
    "# Visualizar os top registos da tabela\n",
    "def visualize_GOOGL():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    print('### Antes do pré-processamento ###')\n",
    "    print(df.head()) #mostra só os primeiros 5 registos\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print('### Após o pré-processamento ###')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função load_data do lstm.py configurada para aceitar qualquer número de parametros\n",
    "#o último atributo é que fica como label (resultado)\n",
    "#stock é um dataframe do pandas (uma especie de dicionario + matriz)\n",
    "#seq_len é o tamanho da janela a ser utilizada na serie temporal\n",
    "def load_data(df_dados, janela):\n",
    "    qt_atributos = len(df_dados.columns)\n",
    "    mat_dados = df_dados.as_matrix() #converter dataframe para matriz (lista com lista de cada registo)\n",
    "    tam_sequencia = janela + 1\n",
    "    res = []\n",
    "    for i in range(len(mat_dados) - tam_sequencia): #numero de registos - tamanho da sequencia\n",
    "        res.append(mat_dados[i: i + tam_sequencia])\n",
    "    res = np.array(res) #dá como resultado um np com uma lista de matrizes(janela deslizante ao longo da serie)\n",
    "    qt_casos_treino = int(round(0.9 * res.shape[0])) #90% passam a ser casos de treino\n",
    "    train = res[:qt_casos_treino, :]\n",
    "    x_train = train[:, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "    y_train = train[:, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "    x_test = res[qt_casos_treino:, :-1]\n",
    "    y_test = res[qt_casos_treino:, -1][:,-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 2 - Definir a topologia da rede (arquitectura do modelo) e compilar '''\n",
    "def build_model2(janela):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(janela, 3), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, input_shape=(janela, 3), return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=\"uniform\"))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime um grafico com os valores de teste e com as correspondentes tabela de previsões\n",
    "def print_series_prediction(y_test,predic):\n",
    "    diff=[]\n",
    "    racio=[]\n",
    "    for i in range(len(y_test)): #para imprimir tabela de previsoes\n",
    "        racio.append( (y_test[i]/predic[i])-1)\n",
    "        diff.append( abs(y_test[i]-predic[i]))\n",
    "        print('valor: %f ---> Previsão: %f Diff: %f Racio: %f' % (y_test[i],predic[i],diff[i],racio[i]))\n",
    "    plt.plot(y_test,color='blue', label='y_test')\n",
    "    plt.plot(predic,color='red', label='prediction') #este deu uma linha em branco\n",
    "    plt.plot(diff,color='green', label='diff')\n",
    "    plt.plot(racio,color='yellow', label='racio')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util para visualizar a topologia da rede num ficheiro em pdf ou png\n",
    "def print_model(model,fich):\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file=fich, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MSE- (Mean square error), RMSE- (root mean square error) –\n",
    "o significado de RMSE depende do range da label. para o mesmo range menor é melhor.\n",
    "'''\n",
    "def LSTM_utilizando_GOOGL_data():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print(\"df\", df.shape)\n",
    "    janela = 22 #tamanho da Janela deslizante\n",
    "    X_train, y_train, X_test, y_test = load_data(df[::-1], janela)# o df[::-1] é o df por ordem inversa\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    #model = build_model(janela)\n",
    "    model = build_model2(janela)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    print_model(model,\"lstm_model.png\")\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #para transformar uma matriz de uma coluna e n linhas em um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Antes do pré-processamento ###\n",
      "         Open        High       Close\n",
      "0  929.000000  935.900024  924.520020\n",
      "1  890.000000  893.380005  891.440002\n",
      "2  891.390015  892.989990  889.140015\n",
      "3  882.260010  892.250000  888.840027\n",
      "4  868.440002  879.960022  878.929993\n",
      "### Após o pré-processamento ###\n",
      "     Open    High   Close\n",
      "0  9.2900  9.3590  9.2452\n",
      "1  8.9000  8.9338  8.9144\n",
      "2  8.9139  8.9299  8.8914\n",
      "3  8.8226  8.9225  8.8884\n",
      "4  8.6844  8.7996  8.7893\n",
      "df (1350, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/campos/anaconda3/envs/myEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (1194, 22, 3)\n",
      "y_train (1194,)\n",
      "X_test (133, 22, 3)\n",
      "y_test (133,)\n",
      "Train on 1074 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "1074/1074 [==============================] - 33s 31ms/step - loss: 54.6070 - acc: 0.0000e+00 - val_loss: 57.5957 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 54.1374 - acc: 0.0000e+00 - val_loss: 57.0660 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 53.6271 - acc: 0.0000e+00 - val_loss: 56.4425 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 53.0120 - acc: 0.0000e+00 - val_loss: 55.6395 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 52.2080 - acc: 0.0000e+00 - val_loss: 54.5500 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 51.1285 - acc: 0.0000e+00 - val_loss: 53.1437 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 49.7615 - acc: 0.0000e+00 - val_loss: 51.5040 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 48.2010 - acc: 0.0000e+00 - val_loss: 49.7338 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 46.5129 - acc: 0.0000e+00 - val_loss: 47.8690 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 44.7252 - acc: 0.0000e+00 - val_loss: 45.8112 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1074/1074 [==============================] - 10s 10ms/step - loss: 42.7533 - acc: 0.0000e+00 - val_loss: 43.5668 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 40.6312 - acc: 0.0000e+00 - val_loss: 41.1759 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 38.3640 - acc: 0.0000e+00 - val_loss: 38.6190 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 35.9544 - acc: 0.0000e+00 - val_loss: 35.9090 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 33.4142 - acc: 0.0000e+00 - val_loss: 33.1076 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 30.8141 - acc: 0.0000e+00 - val_loss: 30.2408 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 28.1488 - acc: 0.0000e+00 - val_loss: 27.3315 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 25.4751 - acc: 0.0000e+00 - val_loss: 24.4075 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 22.8098 - acc: 0.0000e+00 - val_loss: 21.5144 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 20.1865 - acc: 0.0000e+00 - val_loss: 18.6977 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 17.6564 - acc: 0.0000e+00 - val_loss: 15.9926 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 15.2770 - acc: 0.0000e+00 - val_loss: 13.4438 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 13.0401 - acc: 0.0000e+00 - val_loss: 11.1001 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 11.0248 - acc: 0.0000e+00 - val_loss: 8.9723 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 9.2323 - acc: 0.0000e+00 - val_loss: 7.0957 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 7.7028 - acc: 0.0000e+00 - val_loss: 5.4708 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 6.4054 - acc: 0.0000e+00 - val_loss: 4.1113 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 5.3627 - acc: 0.0000e+00 - val_loss: 3.0088 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 4.5517 - acc: 0.0000e+00 - val_loss: 2.1475 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.9716 - acc: 0.0000e+00 - val_loss: 1.4927 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.5668 - acc: 0.0000e+00 - val_loss: 1.0213 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.3196 - acc: 0.0000e+00 - val_loss: 0.6984 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1854 - acc: 0.0000e+00 - val_loss: 0.4904 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1270 - acc: 0.0000e+00 - val_loss: 0.3660 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.2940 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1168 - acc: 0.0000e+00 - val_loss: 0.2507 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1268 - acc: 0.0000e+00 - val_loss: 0.2266 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1390 - acc: 0.0000e+00 - val_loss: 0.2175 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1443 - acc: 0.0000e+00 - val_loss: 0.2162 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1448 - acc: 0.0000e+00 - val_loss: 0.2200 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1417 - acc: 0.0000e+00 - val_loss: 0.2276 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1364 - acc: 0.0000e+00 - val_loss: 0.2335 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1325 - acc: 0.0000e+00 - val_loss: 0.2397 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1291 - acc: 0.0000e+00 - val_loss: 0.2485 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1252 - acc: 0.0000e+00 - val_loss: 0.2570 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1219 - acc: 0.0000e+00 - val_loss: 0.2622 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1200 - acc: 0.0000e+00 - val_loss: 0.2673 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1186 - acc: 0.0000e+00 - val_loss: 0.2706 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.2720 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1169 - acc: 0.0000e+00 - val_loss: 0.2776 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1156 - acc: 0.0000e+00 - val_loss: 0.2856 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.2939 - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3007 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3090 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3223 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3349 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3338 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3322 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3321 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3352 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3404 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3315 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3237 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3117 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "1074/1074 [==============================] - 11s 11ms/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3002 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2955 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1123 - acc: 0.0000e+00 - val_loss: 0.2974 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3062 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3160 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3296 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3348 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3384 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3484 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3475 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3381 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3319 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3268 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "1074/1074 [==============================] - 11s 11ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3209 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3194 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3158 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3228 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3320 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3436 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3485 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3491 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3428 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3446 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3533 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3605 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "1074/1074 [==============================] - 15s 14ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3798 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3879 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3828 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3772 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3719 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3599 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3504 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3411 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3503 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3526 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3470 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "1074/1074 [==============================] - 10s 10ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3224 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.2989 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.2996 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "1074/1074 [==============================] - 10s 10ms/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2981 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3015 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3078 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3122 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "1074/1074 [==============================] - 10s 10ms/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3188 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3331 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3416 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3602 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3717 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3729 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3594 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3352 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3262 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3047 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3044 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3140 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3185 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3162 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3206 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3269 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3342 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3553 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3319 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3187 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3116 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3066 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3035 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3076 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3176 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3342 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3342 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3358 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3214 - val_acc: 0.0000e+00\n",
      "Epoch 165/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3100 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3162 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3605 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3715 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3755 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3756 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3514 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3300 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3108 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2936 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.2916 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3049 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3266 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3435 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3643 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3861 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3897 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3757 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3569 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3345 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3188 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3071 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3022 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.2980 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2947 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.2904 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2913 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.3028 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3304 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3680 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3661 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3611 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3501 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3496 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3443 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3370 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3433 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3582 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3605 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3511 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3359 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3461 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3426 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3432 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3478 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3501 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3437 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3356 - val_acc: 0.0000e+00\n",
      "Epoch 221/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3313 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3393 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3637 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3874 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3901 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3743 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3496 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3377 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3427 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3606 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3686 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3583 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3495 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3439 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3285 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3051 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2860 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1152 - acc: 0.0000e+00 - val_loss: 0.2765 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1152 - acc: 0.0000e+00 - val_loss: 0.2902 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3092 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3325 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3685 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3900 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3987 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.4023 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4030 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.3976 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3864 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3653 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3431 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3250 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3249 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3464 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3630 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3650 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3798 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3755 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3691 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3742 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3716 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3598 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3604 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3708 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3642 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3313 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3037 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.2882 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.2853 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1136 - acc: 0.0000e+00 - val_loss: 0.2929 - val_acc: 0.0000e+00\n",
      "Epoch 277/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.2997 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3051 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3224 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "1074/1074 [==============================] - 13s 13ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3430 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3542 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3572 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3582 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3488 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3374 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3204 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.2954 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.2758 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "1074/1074 [==============================] - 23s 22ms/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.2716 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.1166 - acc: 0.0000e+00 - val_loss: 0.2900 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3253 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3685 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3931 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.4037 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3870 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3727 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3607 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3412 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3193 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3272 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3561 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "1074/1074 [==============================] - 10s 10ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3579 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3728 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3800 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3652 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3525 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3386 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3330 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3401 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3530 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3640 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3738 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3595 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3417 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3043 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.2994 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3141 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3304 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3697 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3619 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3666 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3636 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3526 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3344 - val_acc: 0.0000e+00\n",
      "Epoch 333/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3279 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3312 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3418 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1078 - acc: 0.0000e+00 - val_loss: 0.3616 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3903 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.4105 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.4128 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1136 - acc: 0.0000e+00 - val_loss: 0.4187 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4174 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.4003 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3809 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3598 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3058 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.2912 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2916 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3078 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3244 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "1074/1074 [==============================] - 6s 5ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3314 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3507 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "1074/1074 [==============================] - 6s 6ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3690 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3679 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3574 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3326 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3259 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3387 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3277 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3072 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2920 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1141 - acc: 0.0000e+00 - val_loss: 0.2733 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1170 - acc: 0.0000e+00 - val_loss: 0.2745 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1159 - acc: 0.0000e+00 - val_loss: 0.2957 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3269 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1072 - acc: 0.0000e+00 - val_loss: 0.3568 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3836 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3883 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3996 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3961 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3871 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3710 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3631 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3560 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3376 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3245 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "1074/1074 [==============================] - 9s 8ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3210 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3152 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3073 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3177 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "1074/1074 [==============================] - 10s 9ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3466 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3739 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3944 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3929 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "1074/1074 [==============================] - 10s 10ms/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3670 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3624 - val_acc: 0.0000e+00\n",
      "Epoch 389/500\n",
      "1074/1074 [==============================] - 8s 8ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "1074/1074 [==============================] - 9s 9ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3565 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3725 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3692 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3367 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3089 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.2847 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2806 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2901 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3024 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.3261 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3359 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3248 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3029 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3024 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "1074/1074 [==============================] - 11s 10ms/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3036 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3221 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3334 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3420 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3444 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3307 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3302 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3324 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3532 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3678 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3832 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3979 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.4150 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4290 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "1074/1074 [==============================] - 4s 3ms/step - loss: 3.1161 - acc: 0.0000e+00 - val_loss: 0.4232 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3912 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3360 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3043 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "1074/1074 [==============================] - 4s 4ms/step - loss: 3.1142 - acc: 0.0000e+00 - val_loss: 0.2830 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "1074/1074 [==============================] - 5s 4ms/step - loss: 3.1144 - acc: 0.0000e+00 - val_loss: 0.2945 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "1074/1074 [==============================] - 5s 5ms/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3238 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3635 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "1074/1074 [==============================] - 14s 13ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3771 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "1074/1074 [==============================] - 16s 15ms/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3878 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "1074/1074 [==============================] - 16s 15ms/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3959 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "1074/1074 [==============================] - 15s 14ms/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3916 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3650 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3293 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.2879 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1165 - acc: 0.0000e+00 - val_loss: 0.2610 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1206 - acc: 0.0000e+00 - val_loss: 0.2660 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1185 - acc: 0.0000e+00 - val_loss: 0.2781 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.3113 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3525 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3602 - val_acc: 0.0000e+00\n",
      "Epoch 445/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3406 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3389 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3467 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3550 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3586 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3619 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3759 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3702 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3308 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3306 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3414 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3407 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "1074/1074 [==============================] - 13s 12ms/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3479 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3717 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3799 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3878 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "1074/1074 [==============================] - 12s 12ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3880 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "1074/1074 [==============================] - 12s 11ms/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3668 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3332 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3082 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3031 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.3189 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3483 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3821 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "1074/1074 [==============================] - 8s 7ms/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.4015 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.4042 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3683 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3188 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.2833 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1158 - acc: 0.0000e+00 - val_loss: 0.2684 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.2878 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.3280 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3820 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.4108 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1140 - acc: 0.0000e+00 - val_loss: 0.4150 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1138 - acc: 0.0000e+00 - val_loss: 0.3935 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3669 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3365 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1076 - acc: 0.0000e+00 - val_loss: 0.3094 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2839 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1153 - acc: 0.0000e+00 - val_loss: 0.2732 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1164 - acc: 0.0000e+00 - val_loss: 0.2872 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3173 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3545 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3718 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3896 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.4132 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.4318 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1177 - acc: 0.0000e+00 - val_loss: 0.4287 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "1074/1074 [==============================] - 7s 7ms/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3877 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "1074/1074 [==============================] - 7s 6ms/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3551 - val_acc: 0.0000e+00\n",
      "Train Score: 2.83 MSE (1.68 RMSE)\n",
      "Test Score: 1.28 MSE (1.13 RMSE)\n",
      "['loss', 'acc']\n",
      "valor: 8.068400 ---> Previsão: 7.169452 Diff: 0.898949 Racio: 0.125386\n",
      "valor: 8.214900 ---> Previsão: 7.169452 Diff: 1.045448 Racio: 0.145820\n",
      "valor: 8.268400 ---> Previsão: 7.169452 Diff: 1.098949 Racio: 0.153282\n",
      "valor: 8.216300 ---> Previsão: 7.169452 Diff: 1.046848 Racio: 0.146015\n",
      "valor: 8.240600 ---> Previsão: 7.169452 Diff: 1.071148 Racio: 0.149404\n",
      "valor: 8.357400 ---> Previsão: 7.169452 Diff: 1.187948 Racio: 0.165696\n",
      "valor: 8.285500 ---> Previsão: 7.169452 Diff: 1.116048 Racio: 0.155667\n",
      "valor: 8.221000 ---> Previsão: 7.169452 Diff: 1.051548 Racio: 0.146671\n",
      "valor: 8.173500 ---> Previsão: 7.169452 Diff: 1.004048 Racio: 0.140045\n",
      "valor: 8.195600 ---> Previsão: 7.169452 Diff: 1.026148 Racio: 0.143128\n",
      "valor: 8.099000 ---> Previsão: 7.169452 Diff: 0.929549 Racio: 0.129654\n",
      "valor: 8.054800 ---> Previsão: 7.169452 Diff: 0.885348 Racio: 0.123489\n",
      "valor: 7.884200 ---> Previsão: 7.169452 Diff: 0.714748 Racio: 0.099694\n",
      "valor: 7.821900 ---> Previsão: 7.169452 Diff: 0.652448 Racio: 0.091004\n",
      "valor: 7.811000 ---> Previsão: 7.169452 Diff: 0.641548 Racio: 0.089484\n",
      "valor: 8.020300 ---> Previsão: 7.169452 Diff: 0.850849 Racio: 0.118677\n",
      "valor: 8.119800 ---> Previsão: 7.169452 Diff: 0.950348 Racio: 0.132555\n",
      "valor: 8.055900 ---> Previsão: 7.169452 Diff: 0.886449 Racio: 0.123642\n",
      "valor: 7.802900 ---> Previsão: 7.169452 Diff: 0.633448 Racio: 0.088354\n",
      "valor: 7.717500 ---> Previsão: 7.169452 Diff: 0.548048 Racio: 0.076442\n",
      "valor: 7.532200 ---> Previsão: 7.169452 Diff: 0.362748 Racio: 0.050596\n",
      "valor: 7.751600 ---> Previsão: 7.169452 Diff: 0.582148 Racio: 0.081198\n",
      "valor: 7.799800 ---> Previsão: 7.169452 Diff: 0.630348 Racio: 0.087921\n",
      "valor: 7.861600 ---> Previsão: 7.169452 Diff: 0.692148 Racio: 0.096541\n",
      "valor: 7.759700 ---> Previsão: 7.169452 Diff: 0.590248 Racio: 0.082328\n",
      "valor: 7.848000 ---> Previsão: 7.169452 Diff: 0.678548 Racio: 0.094644\n",
      "valor: 7.850000 ---> Previsão: 7.169452 Diff: 0.680548 Racio: 0.094923\n",
      "valor: 7.790000 ---> Previsão: 7.169452 Diff: 0.620548 Racio: 0.086554\n",
      "valor: 7.802300 ---> Previsão: 7.169452 Diff: 0.632848 Racio: 0.088270\n",
      "valor: 7.857900 ---> Previsão: 7.169452 Diff: 0.688448 Racio: 0.096025\n",
      "valor: 7.894400 ---> Previsão: 7.169452 Diff: 0.724948 Racio: 0.101116\n",
      "valor: 7.758800 ---> Previsão: 7.169452 Diff: 0.589348 Racio: 0.082203\n",
      "valor: 7.643300 ---> Previsão: 7.169452 Diff: 0.473848 Racio: 0.066093\n",
      "valor: 7.644600 ---> Previsão: 7.169452 Diff: 0.475149 Racio: 0.066274\n",
      "valor: 7.782200 ---> Previsão: 7.169452 Diff: 0.612748 Racio: 0.085467\n",
      "valor: 7.761800 ---> Previsão: 7.169452 Diff: 0.592348 Racio: 0.082621\n",
      "valor: 7.914700 ---> Previsão: 7.169452 Diff: 0.745248 Racio: 0.103948\n",
      "valor: 7.951700 ---> Previsão: 7.169452 Diff: 0.782248 Racio: 0.109108\n",
      "valor: 8.094500 ---> Previsão: 7.169452 Diff: 0.925048 Racio: 0.129026\n",
      "valor: 8.079000 ---> Previsão: 7.169452 Diff: 0.909549 Racio: 0.126864\n",
      "valor: 8.153400 ---> Previsão: 7.169452 Diff: 0.983949 Racio: 0.137242\n",
      "valor: 8.178900 ---> Previsão: 7.169452 Diff: 1.009448 Racio: 0.140799\n",
      "valor: 8.156500 ---> Previsão: 7.169452 Diff: 0.987049 Racio: 0.137674\n",
      "valor: 8.098400 ---> Previsão: 7.169452 Diff: 0.928949 Racio: 0.129570\n",
      "valor: 8.125000 ---> Previsão: 7.169452 Diff: 0.955548 Racio: 0.133281\n",
      "valor: 8.152000 ---> Previsão: 7.169452 Diff: 0.982548 Racio: 0.137047\n",
      "valor: 8.122000 ---> Previsão: 7.169452 Diff: 0.952548 Racio: 0.132862\n",
      "valor: 8.096800 ---> Previsão: 7.169452 Diff: 0.927348 Racio: 0.129347\n",
      "valor: 8.078000 ---> Previsão: 7.169452 Diff: 0.908548 Racio: 0.126725\n",
      "valor: 8.099300 ---> Previsão: 7.169452 Diff: 0.929848 Racio: 0.129696\n",
      "valor: 8.045700 ---> Previsão: 7.169452 Diff: 0.876248 Racio: 0.122220\n",
      "valor: 8.028800 ---> Previsão: 7.169452 Diff: 0.859348 Racio: 0.119862\n",
      "valor: 7.924500 ---> Previsão: 7.169452 Diff: 0.755048 Racio: 0.105315\n",
      "valor: 8.080100 ---> Previsão: 7.169452 Diff: 0.910648 Racio: 0.127018\n",
      "valor: 8.077700 ---> Previsão: 7.169452 Diff: 0.908248 Racio: 0.126683\n",
      "valor: 8.130200 ---> Previsão: 7.169452 Diff: 0.960748 Racio: 0.134006\n",
      "valor: 8.252100 ---> Previsão: 7.169452 Diff: 1.082649 Racio: 0.151009\n",
      "valor: 8.271800 ---> Previsão: 7.169452 Diff: 1.102348 Racio: 0.153756\n",
      "valor: 8.260100 ---> Previsão: 7.169452 Diff: 1.090648 Racio: 0.152124\n",
      "valor: 8.298600 ---> Previsão: 7.169452 Diff: 1.129148 Racio: 0.157494\n",
      "valor: 8.295300 ---> Previsão: 7.169452 Diff: 1.125849 Racio: 0.157034\n",
      "valor: 8.309400 ---> Previsão: 7.169452 Diff: 1.139948 Racio: 0.159001\n",
      "valor: 8.274600 ---> Previsão: 7.169452 Diff: 1.105149 Racio: 0.154147\n",
      "valor: 8.290200 ---> Previsão: 7.169452 Diff: 1.120748 Racio: 0.156323\n",
      "valor: 8.243700 ---> Previsão: 7.169452 Diff: 1.074248 Racio: 0.149837\n",
      "valor: 8.281700 ---> Previsão: 7.169452 Diff: 1.112248 Racio: 0.155137\n",
      "valor: 8.444300 ---> Previsão: 7.169452 Diff: 1.274848 Racio: 0.177817\n",
      "valor: 8.495300 ---> Previsão: 7.169452 Diff: 1.325849 Racio: 0.184930\n",
      "valor: 8.584500 ---> Previsão: 7.169452 Diff: 1.415048 Racio: 0.197372\n",
      "valor: 8.569800 ---> Previsão: 7.169452 Diff: 1.400348 Racio: 0.195322\n",
      "valor: 8.450300 ---> Previsão: 7.169452 Diff: 1.280849 Racio: 0.178654\n",
      "valor: 8.238300 ---> Previsão: 7.169452 Diff: 1.068848 Racio: 0.149084\n",
      "valor: 8.201900 ---> Previsão: 7.169452 Diff: 1.032448 Racio: 0.144007\n",
      "valor: 8.152400 ---> Previsão: 7.169452 Diff: 0.982948 Racio: 0.137102\n",
      "valor: 8.182600 ---> Previsão: 7.169452 Diff: 1.013148 Racio: 0.141315\n",
      "valor: 8.201300 ---> Previsão: 7.169452 Diff: 1.031848 Racio: 0.143923\n",
      "valor: 8.216200 ---> Previsão: 7.169452 Diff: 1.046748 Racio: 0.146001\n",
      "valor: 8.292300 ---> Previsão: 7.169452 Diff: 1.122848 Racio: 0.156616\n",
      "valor: 8.298800 ---> Previsão: 7.169452 Diff: 1.129348 Racio: 0.157522\n",
      "valor: 8.300600 ---> Previsão: 7.169452 Diff: 1.131148 Racio: 0.157773\n",
      "valor: 8.348500 ---> Previsão: 7.169452 Diff: 1.179048 Racio: 0.164454\n",
      "valor: 8.389600 ---> Previsão: 7.169452 Diff: 1.220149 Racio: 0.170187\n",
      "valor: 8.400300 ---> Previsão: 7.169452 Diff: 1.230849 Racio: 0.171680\n",
      "valor: 8.373200 ---> Previsão: 7.169452 Diff: 1.203748 Racio: 0.167900\n",
      "valor: 8.421700 ---> Previsão: 7.169452 Diff: 1.252248 Racio: 0.174664\n",
      "valor: 8.465500 ---> Previsão: 7.169452 Diff: 1.296048 Racio: 0.180774\n",
      "valor: 8.492700 ---> Previsão: 7.169452 Diff: 1.323248 Racio: 0.184568\n",
      "valor: 8.513600 ---> Previsão: 7.169452 Diff: 1.344148 Racio: 0.187483\n",
      "valor: 8.510000 ---> Previsão: 7.169452 Diff: 1.340548 Racio: 0.186981\n",
      "valor: 8.478100 ---> Previsão: 7.169452 Diff: 1.308648 Racio: 0.182531\n",
      "valor: 8.496700 ---> Previsão: 7.169452 Diff: 1.327248 Racio: 0.185125\n",
      "valor: 8.449300 ---> Previsão: 7.169452 Diff: 1.279848 Racio: 0.178514\n",
      "valor: 8.567500 ---> Previsão: 7.169452 Diff: 1.398048 Racio: 0.195001\n",
      "valor: 8.498500 ---> Previsão: 7.169452 Diff: 1.329048 Racio: 0.185377\n",
      "valor: 8.490800 ---> Previsão: 7.169452 Diff: 1.321348 Racio: 0.184303\n",
      "valor: 8.472700 ---> Previsão: 7.169452 Diff: 1.303248 Racio: 0.181778\n",
      "valor: 8.511500 ---> Previsão: 7.169452 Diff: 1.342049 Racio: 0.187190\n",
      "valor: 8.536400 ---> Previsão: 7.169452 Diff: 1.366948 Racio: 0.190663\n",
      "valor: 8.578400 ---> Previsão: 7.169452 Diff: 1.408949 Racio: 0.196521\n",
      "valor: 8.614100 ---> Previsão: 7.169452 Diff: 1.444648 Racio: 0.201500\n",
      "valor: 8.645800 ---> Previsão: 7.169452 Diff: 1.476348 Racio: 0.205922\n",
      "valor: 8.659100 ---> Previsão: 7.169452 Diff: 1.489648 Racio: 0.207777\n",
      "valor: 8.683900 ---> Previsão: 7.169452 Diff: 1.514448 Racio: 0.211236\n",
      "valor: 8.700000 ---> Previsão: 7.169452 Diff: 1.530548 Racio: 0.213482\n",
      "valor: 8.723700 ---> Previsão: 7.169452 Diff: 1.554248 Racio: 0.216788\n",
      "valor: 8.679100 ---> Previsão: 7.169452 Diff: 1.509648 Racio: 0.210567\n",
      "valor: 8.501400 ---> Previsão: 7.169452 Diff: 1.331948 Racio: 0.185781\n",
      "valor: 8.498000 ---> Previsão: 7.169452 Diff: 1.328548 Racio: 0.185307\n",
      "valor: 8.396500 ---> Previsão: 7.169452 Diff: 1.227049 Racio: 0.171150\n",
      "valor: 8.351400 ---> Previsão: 7.169452 Diff: 1.181948 Racio: 0.164859\n",
      "valor: 8.385100 ---> Previsão: 7.169452 Diff: 1.215648 Racio: 0.169559\n",
      "valor: 8.406300 ---> Previsão: 7.169452 Diff: 1.236848 Racio: 0.172516\n",
      "valor: 8.498700 ---> Previsão: 7.169452 Diff: 1.329248 Racio: 0.185404\n",
      "valor: 8.494800 ---> Previsão: 7.169452 Diff: 1.325348 Racio: 0.184860\n",
      "valor: 8.478000 ---> Previsão: 7.169452 Diff: 1.308548 Racio: 0.182517\n",
      "valor: 8.567500 ---> Previsão: 7.169452 Diff: 1.398048 Racio: 0.195001\n",
      "valor: 8.525700 ---> Previsão: 7.169452 Diff: 1.356248 Racio: 0.189170\n",
      "valor: 8.489100 ---> Previsão: 7.169452 Diff: 1.319648 Racio: 0.184065\n",
      "valor: 8.451000 ---> Previsão: 7.169452 Diff: 1.281548 Racio: 0.178751\n",
      "valor: 8.421000 ---> Previsão: 7.169452 Diff: 1.251548 Racio: 0.174567\n",
      "valor: 8.417000 ---> Previsão: 7.169452 Diff: 1.247548 Racio: 0.174009\n",
      "valor: 8.398800 ---> Previsão: 7.169452 Diff: 1.229348 Racio: 0.171470\n",
      "valor: 8.414600 ---> Previsão: 7.169452 Diff: 1.245149 Racio: 0.173674\n",
      "valor: 8.401800 ---> Previsão: 7.169452 Diff: 1.232348 Racio: 0.171889\n",
      "valor: 8.551300 ---> Previsão: 7.169452 Diff: 1.381848 Racio: 0.192741\n",
      "valor: 8.539900 ---> Previsão: 7.169452 Diff: 1.370448 Racio: 0.191151\n",
      "valor: 8.565100 ---> Previsão: 7.169452 Diff: 1.395648 Racio: 0.194666\n",
      "valor: 8.600800 ---> Previsão: 7.169452 Diff: 1.431348 Racio: 0.199645\n",
      "valor: 8.589500 ---> Previsão: 7.169452 Diff: 1.420048 Racio: 0.198069\n",
      "valor: 8.789300 ---> Previsão: 7.169452 Diff: 1.619848 Racio: 0.225938\n",
      "valor: 8.888400 ---> Previsão: 7.169452 Diff: 1.718949 Racio: 0.239760\n",
      "valor: 8.891400 ---> Previsão: 7.169452 Diff: 1.721948 Racio: 0.240179\n",
      "valor: 8.914400 ---> Previsão: 7.169452 Diff: 1.744948 Racio: 0.243387\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4VFX6wPHvIR1IKKEoNREwQCAJGIrCgoKArAiKqKwCArKKumJvi2tZV9ddu/wUxILgonQRu9IUCwhIJ/SEDkko6X3e3x9nQhJIYIAkM0Pez/PcJ1PuzH3vyb3vnHvuuecaEUEppZT3qObuAJRSSp0dTdxKKeVlNHErpZSX0cStlFJeRhO3Ukp5GU3cSinlZTRxK6WUl9HErZRSXkYTt1JKeRnfivjSevXqSVhYWEV8tVJKXZBWr16dLCL1XZm3QhJ3WFgYq1atqoivVkqpC5IxZrer82pTiVJKeRlN3Eop5WU0cSullJepkDbu0uTl5bFv3z6ys7Mra5EXvMDAQJo0aYKfn5+7Q1FKVaJKS9z79u0jODiYsLAwjDGVtdgLlohw5MgR9u3bR3h4uLvDUUpVokprKsnOziY0NFSTdjkxxhAaGqpHMEpVQZXaxq1Ju3xpeSpVNVVaU4lSSl2IHA7YuhV++w2SkuDxxyt+mdqr5CQJCQl88skn5/z5F198sRyjUUp5oiVLICwMatQAPz9o2xbuuAMmTLCJvKJp4j6JJm6l1Ol88w38+c8QFAR33w1PPAEffghxcbBnD1SrhKxaZZpK/vGPf1CvXj3uv/9+AMaPH0/Dhg0ZN25cifmeeOIJ4uLiiImJ4fbbb2fcuHE88cQTLF26lJycHO69917uuusuDh48yC233EJqair5+flMnDiRr776iqysLGJiYoiMjGT69OnuWFWlVAX54QcYNAjatYPvv4d69dwThxGRcv/S2NhYOXmskri4ONq0aQPAAw/A2rXlu8yYGHjjjbLfT0hIYPDgwfzxxx84HA5atWrF77//TmhoaIn5li5dyiuvvMKXX34JwOTJk0lMTOSpp54iJyeHbt26MXv2bObNm0d2djbjx4+noKCAzMxMgoODqVmzJunp6eW7cqdRvFyVcpeMDNiyxSa0gAB3R1MxCgrs+onA8uVQu3b5fr8xZrWIxLoyb5WpcYeFhREaGsqaNWs4fPgwHTp0OCVpl+b7779n/fr1zJkzB4CUlBS2b99Op06dGD16NHl5eVx//fXExMRU9Coo5RHS0uyJuHXrbLJet85WxAoKoGFDGDsW7rsPXNi9vMr06XZ958wp/6R9ttySuE9XM65IY8aM4aOPPuLQoUOMHj3apc+ICBMmTKBfv36nvPfTTz/x1VdfMXz4cB599FFGjBhR3iEr5VZ5efZE3MKFsGsX7NgBGzfaJA02UUdG2nbeiAiYOROee842Kfz8M1woPVbz8uDZZ6FDBxg82N3RVKEaN8ANN9zA008/TV5eXpknIIODg0lLSzvxvF+/fkycOJFevXrh5+fHtm3baNy4McnJyTRu3Ji//vWvZGRk8McffzBixAj8/PzIy8vTy9C9xJEjUL26PdGkrJwcm3jnzIEFC+DYMfD3h/BwO113HfToAbGxUKdOyc8OHw7vvw9//St88gncdpt71qE8bNwIX39tt43duyE+Hr76yjN+jKpU4vb39+eqq66idu3a+Pj4lDpPVFQUvr6+REdHM3LkSO6//34SEhLo2LEjIkL9+vWZP38+S5cu5eWXX8bPz4+aNWsybdo0AO68806ioqLo2LGjnpz0YCLwyiu2z60INGkC998Pjzzi7sjKh8Nh1y0gAMaPd+2HafNmmDwZpk6F48ehVi0YOBCGDIG+fSEw0LVljx4NkybBY4/ZE3k1a57fuhQqrPUmJtofiD/9qXyS6OLF9ge8aVNIToYff4Rvv7WJu7hu3aB///NfXrkQkXKfLrvsMjnZ5s2bT3mtshUUFEh0dLRs27bN3aGUG08oV2+Tlydy110iIHLDDSL//KdIr172+QcfuDu6U+XkiKxcKTJxosiDD4o8/rjIc8+JfPutSHa2nefYMZE9e4o+8/jjdn1ApHVrka+/Ftm2zc5XKCtLZNYskREjRMLC7Lx+fiJDh9r5c3LOPeZffrHfd9ddIosXi6xbJ+JwuPbZY8dEfv1VZO1akfh4kfx8+1rv3vY7q1e3f1u2FPnXv0qu99maN6+onAonf3+Rnj1F/u//RA4eFDl8WGT16pJlVxGAVeJijq0yiXvTpk0SHh4uDz30kFvjKG/uLldvkpcn8tFHIq1a2S3/ySdFCgrse7m5In36iPj6ivzwg3vjFBFJSRGZMEHkmmtEatQoSipBQSIBAUXPg4NFmjUret63r8hjj9nHY8eKfP+9SNOmJRNTw4YiV14pUqeOfV6/vsiNN4q89ZZNUuVl5MiSy33wwTMn73XrRBo1Kvm5wECR0FD7v/noI5H0dJGpU+06gIgxIldfLTJtmkhqquvxHTwoUq+eSIcOImvWiHz5pcjSpSKZmee33ufqbBK3W7oDeoINGzYwfPjwEq8FBASwYsUKN0V0bjytXD2VwwF9+tjD4pgY+Oc/bVttcSkp0L07HDhg2zNDQio+rvx88PEpOuTfvRveegvee8/23oiIgKuvhp49oVMnaN7czpuVZddlwQJIT4eoKMjNhXfegUOH4NprYf588PW17y9bZpsBDh2yTSKbN0OLFjBqFPTqZWMobw4H/PGH7So4cyZMnGh7m7z55qlNHA6HbVe/+WYIDrYdGHx8bBPGli2wd6/trXLVVSU/t2sXTJtmp/h4exVjjx5wxRX2e0JCbBlGRtpy2LrVll3z5vDMM3aZf/xhr3x0t7PpDujVNe7C2lJVpjVu10ycaGtnr79++lrfqlV2vhdfrJg4kpNFXntNpGtXW/M1RuSii0RuvVXk5ptFfHzsdOuttnnkbGVn22YOd9Uay+JwiDz0kC3bK68U+eIL2wzy2mu2CaRWLfte27bn1vThcIgsWyby6KP2O05u/ihrev31cl/Vc0ZVqHHv22dPUjRuDA0aeMaZXneo6jVukTP/7/fvtzWqTp1sDetM8//5z7ByJSQk2LEoykNcnD0Z+r//2Zpxp04QHQ0XX2y72C1aZHtz3HmnrZU2bVo+y/UkInYsj//+1/5PCrVrZ0/8depkT4TWqnX+yyoosDXr48ft0cXGjbYGHhFh/6d79tj3hw6tnEvUXXFBXYBT2o6ZmGgP+QIC7CFUSoo99LlQr9iqilJSin6Yq1WDL76ATz+1h86BgZCZCTt3wtGjtilhyBB7aL1tG9Sta5sAateG1FQ7nkRuru3p4MoP/D/+YQ+1J02Chx8+u7iXL7dNAqmpNsaMDNvksX697dlxxx32kD8qquTnCuuAnpJEKoIxMG6c/X/Mm2ebpK67Dlq2LP9l+fjY3iw1a9oeQ337lnz/ssvKf5mVyWMTd3q6/cemptodsVEj+/rx47a2Xbu2baNLTrbJe9MmO0/DhlW39u3tROCpp2DGDNt2WcjPz3YFa9QILr3UJnV/f+jXz9aevvjC9q8Fm/gcDnj6adtevXQpZGfb2q6rCeLyy6F3b3j5ZbjhBrjkkjN/xuGw848fb2uMTZrY/uHVq9u+zoMHwz33QP36pX/emKqz3fr5wS23uDsK7+aRiXvfPluj9vW1l80eO2ZrVoVq1rQXAhhjd4SQEHvos2+fHQ/3ootsYs/MtDWtOnXsd6nSORy2Vlgeh6jn44UX4MUXbV/ZMWNsoj5wwNay+/a1ybS0k2gTJtjLrgMDbZLdvBlefdWekBs9GkaMgC5dzi6W556DK6+0lYPLL7e15L/8pehHZO1aW1mIi7PLW7/eboM33WRPLLq7LNUFztXG8LOZzufk5LFj9qRMYf9NEdtV68ABkcREe9KltJNLDof97KZN9vPFp02b7HeUpxo1aoiIyP79++XGG2887byvv/66ZGRknHjev39/OVZOnULP5+Tk/PkigwfbrlZg+/L+5S/2JE9xWVkiixaJ/Pe/It98I1JsVcrNzJk2hmHDXO/vW9ESEkReeqnoZFfz5rbbXOGJtMI+v+3b2xOL06Z5TuzK+1DeJyeNMQ8CYwABNgCjRKTMmx2e68nJvDxbi/HzgzZtzq29T8TWHjMz7WFqQYHtJuTvbw+VT3f1V0FBQYkrKvPybPeskBBbsy9+KHs2owCGhYWxatUq6lXAGJCunpwUgQ0b7CG8v79ta5wyxZ4E690bWrWytcilS+1Ry5Ahttvc4sXwyy/2xFkhf3/bBtynj+2mFhVlT/wkJtqaZ/v2tsnKFYmJ9mTVhAn25NSiRZ53rkLENsW89JLdHvr2hWuusScXL7lEj+ZU+SjX7oBAYyAeCHI+nwWMPN1nzqXG7XDYK7tWrSr/rkypqSJffhkvzZtHyA03jJDIyPZy4403SkZGhjRv3lyee+456datm3z66aeyY8cO6devn3To0FE6duwus2fHycqVIt99t0s6d+4qsbGx8tRTT52occfHx0tkZKSIiOTn58vDDz8s7dq1k/bt28tbb70lb775pvj5+Um7du3kyiuvFBGR5s2bS1JSkoiIvPrqqxIZGSmRkZHyurNvUnx8vLRu3VrGjBkjbdu2lT59+khmGYXiao37uedK1hKNERk//tQjkfR0O2/h1WnR0bYb15dfihw6ZK/We+QRkZiYkt2q6tUrehwUZC8COXDg1DgcDpFx40QaNxYJD7fzVqsmMny47SqnVFXFWdS4Xa0r+AJBxpg8oDpw4Cx/TEoqbUBugUZZ0NzX1ujO2mkG5A4OtjXK3bu38uyzHxAV1Y033xzNO++8A0BgYCA///wzAL179+attybhcLRizZoVTJhwD/PmLebhh+9n0KC7efLJEbzzztulLmfy5MnEx8ezZs0afH19OXr0KHXr1uW1115jyZIlp9S4V69ezZQpU1ixYgUiQpcuXejZsyd16tRh+/btfPrpp7z33nvcfPPNzJ07l2HDhp1DwdhR2p57Dq6/3p6w273b1qh79Dh13ho17Im9cePsxSEnHyT062cnsLXlFStsLTs+Hlq3thc6TJ9uT9T997/2SKVTJzsGRuPGMHu2vcCkXz/bjTMkxHZ/i4g4p1VTqko6Y+IWkf3GmFeAPUAW8L2IfF/egRhjmzYqir8/NG3alFtv7cbGjXD11cOYO/ctAG5xnuJOT0/n119/5aabbqKgwB6y5+XlEBoKGzb8wn/+M5e0NBg+fDiPl3JH0IULFzJ27Fh8ncfOdevWPW1MP//8MzfccAM1nJ2FBw8ezLJlyxg4cCDh4eEnxvi+7LLLSEhIOKf1PnbMjtAWHm6vLgsOdu1zrow33KCB7c518hWI/fvD3/8O331nT9zNmGGvzps1C+691ybyL7/UJgalztUZdx1jTB1gEBAOHAdmG2OGicj/TprvTuBOgGbNmp3+S8uoGVd0byhjDP7+trfCihWQn2+XWJg4HQ4HtWvXZtq0tTRoAMVXwxjw9TUkJZXdpUtEMGfRp0tOc34hoFhDr4+PD1lZWS5/L9judFOm2As+DhyAX391PWmXh7Ztiy4jHjnS1rA7drTJesoUTdpKnQ9XTv9dDcSLSJKI5AHzgCtOnklEJotIrIjE1i8rs7nZnj17+O2332jQABYt+pSIiO4l3g8JCaFRo3AWLZrNRRfZxLpu3ToAunXrxi+/zOD4cZg6tfThWvv27cukSZPIz88H4OjRo4icOsZ3oR49ejB//nwyMzPJyMjgs88+409/+tOJ91NSbBNEKR89rV277ImzF1+0/Z4XLLC1XHfp1s0Ok1m7Nvz737Y5RSl17lxJ3HuArsaY6sZWJ3sDcRUbVsVo06YNU6dOJSYmiry8owwefDe5ubatNj/f9px45pnpfPPNB3TqFE1kZCSff/45AG+++SaffPI2w4d34uDBlFK/f8yYMTRr1oyoqCiio6N5771PWL8eBg++k/79+3PVVVeRkWGXtWMHVK/ekcGDR9K5c2e6dOnCmDFjiI7uQFKSvWhk+3bbf/34cfvcFQ6H7btcrZodnOe77zxjDOHu3W05P/SQuyNRyvu52h3wOeAWIB9YA4wRkZyy5vfE0QETEhIYMGAAG4uNjp6dbS+aSE0tms8Y252trBOkW7faBN+mje22eLLC7ogHDtirPwMC7Px16tgpIcFeRBIQYJNsZqZ9XLeu7X6YlmbnDw62XeqCg4uW2bbtqV3l4uLiaNq0Ddu22ZgmT7bnfj/80F72rZTyDuU+VomIPAM8c15ReaDAQNvbJC3NDjiTm2tPkJ6uV0ujRnY8jLg42y+8+AnVzEzbYyMjwyb15s1tr4zERHtZ/rFjttdGy5ZFST8lxb538KB9LTDQtq2HhBT1G2/Rwp7k27nT9r4ofvWgiL1LyZIlRcODDhhg25WVUhemKnOKKCwsrERtu5AxNkm6OvZycLDt9rZjh22KKBxtTMQm7Zwcm7BDQ4suIGrY0D7OyrIXwBS/sKhWLbvs0w0wFBBge4Xs2GHbr1u2LErqKSk2aY8fb5/v3m274lWVcS+UqoqqTOIuTzVq2GaJuDjb1NK6tW0WyciwteXSzs2e7nytKwMM1a5tv3vPHpucGzSwPwQpKbZ2/a9/ndcqKaW8iCbuc1TYrXD3btsEkpxcNChWRWnQwLaDHzxolwe2eeXt0q8HUkpdoDRxn4fC9us9e2xPkcaNK+YWUMU1amSba/LzbS39wIGKvXBJKeV5LuBh2yueMXaQpvx82z5dGd3XC9vk69a1vVQu5IH3lVKlq7K7/bPPPssrr7zC008/zcKFCwFYtmwZkZGRxMTEkJWVxaOPPkpkZCSPPvpomd8TEmKbMJo00asBlVKVo8qnmn/+858nHk+fPp1HHnmEUc4O0O+++y5JSUklLj8vzZmu8FdKqfJUpRL3Cy+8wLRp02jatCn169fnsssuY+TIkQwYMIDjx48za9YsvvvuOxYuXEhaWhoZGRl06dKFJ5988sRAVEop5W5uSdwPfPsAaw+tPfOMZyHmohjeuKb0wavADqE6Y8YM1qxZQ35+Ph07duSyYncMHTNmDD///DMDBgxgyJAhgL1ZwtqTh59VSik3qzI17mXLlnHDDTdQ3dkFY+DAgW6OSCmlzo1bEvfpasYV6WyGXFVKKU9VZXqV9OjRg88++4ysrCzS0tL44osv3B2SUkqdE89qKtmzx17HXQE61qjBLb17E9O2Lc0bNeJPUVH26pmUFNi/3w7BV/wx2AFECh97qkOH4O673R2FUgpOewvF8uRZibuCjR87lvFjx5b5/kcvvVTiefoff1R0SEopddY8K3Frh+iz53DA0qXujkIpVYmqTBu3UkpdKDRxK6WUl9HErZRSXkYTt1JKeRlN3Eop5WU0cbtg1apVjBs3zt1hKKUU4GndASuJiCAiVHPxLgSxsbHExsZWcFRKKeWaKlPjTkhIoE2bNtxzzz107NiRO+64g9jYWCIjI3nmmWdOzLdy5UquuOIKoqOj6dy5M2lpaSxdupQBAwYAcPToUa6//nqioqLo2rUr69evd9cqKaWqKDfVuB8Aynu41Bjg9Jeabt26lSlTpvDOO+9w9OhR6tatS0FBAb1792b9+vW0bt2aW265hZkzZ9KpUydSU1MJCgoq8R3PPPMMHTp0YP78+SxevJgRI0bo0K9KqUpVpZpKmjdvTteuXQGYNWsWkydPJj8/n4MHD7J582aMMVx88cV06tQJgJCQkFO+4+eff2bu3LkA9OrViyNHjpCSkkKtWrUqb0WUUlWamxK3e4Z1rVGjBgDx8fG88sorrFy5kjp16jBy5Eiys7MRkTMO/Soip7ymw8UqpSpTlWnjLi41NZUaNWpQq1YtDh8+zDfffANA69atOXDgACtXrgQgLS2N/Pz8Ep/t0aMH06dPB2Dp0qXUq1ev1Jq5UkpVlCrVVFIoOjqaDh06EBkZySWXXEK3bt0A8Pf3Z+bMmdx3331kZWURFBR04g7whZ599llGjRpFVFQU1atXZ+rUqe5YBaVUFWZKO/Q/X7GxsbJq1aoSr8XFxdGmTZtyX1ZVp+Wq1IXBGLNaRFzqd1wlm0qUUsqbaeJWSikvU6mJuyKaZaoyLU+lqqZKS9yBgYEcOXJEk005ERGOHDlCYGCgu0NRSlWySutV0qRJE/bt20dSUlJlLfKCFxgYSJMmTdwdhlKqklVa4vbz8yM8PLyyFqeUUhcsPTmplFJexqXEbYypbYyZY4zZYoyJM8ZcXtGBKaWUKp2rTSVvAt+KyBBjjD9QvQJjUkopdRpnTNzGmBCgBzASQERygdyKDUsppVRZXGkquQRIAqYYY9YYY943xtQ4eSZjzJ3GmFXGmFXac0QppSqOK4nbF+gITBSRDkAG8MTJM4nIZBGJFZHY+vXrl3OYSimlCrmSuPcB+0RkhfP5HGwiV0op5QZnTNwicgjYa4yJcL7UG9hcoVEppZQqk6u9Su4Dpjt7lOwCRlVcSEoppU7HpcQtImsBl8aJVUopVbH0ykmllPIymriVUsrLaOJWSikvo4lbKaW8jCZupZTyMpq4lVLKy2jiVkopL6OJWymlvIwmbqWU8jKauJVSysto4lZKKS+jiVsppbyMJm6llPIymriVUsrLaOJWSikvo4lbKaW8jCZupZTyMpq4lVLKy2jiVkopL6OJWymlvIwmbqWU8jKauJVSysto4lZKKS+jiVsppbyMJm6llPIymriVUsrLaOJWSikvo4lbKaW8jCZupZTyMpq4lVLKy2jiVkopL6OJWymlvIwmbqWU8jKauJVSysto4lZKKS/jcuI2xvgYY9YYY76syICUUkqd3tnUuO8H4ioqEKWUUq5xKXEbY5oA1wLvV2w4SimlzsTVGvcbwGOAowJjUUop5YIzJm5jzAAgUURWn2G+O40xq4wxq5KSksotQKWUUiW5UuPuBgw0xiQAM4Bexpj/nTyTiEwWkVgRia1fv345h6mUUqrQGRO3iDwpIk1EJAwYCiwWkWEVHplSSqlSaT9upZTyMr5nM7OILAWWVkgkSimlXKI1bqWU8jKauJVSysto4lZKKS+jiVsppbyMJm6llPIymriVUsrLaOJWSikvo4lbKaW8jCZupZTyMpq4lVLKy2jiVkopL6OJWymlvIwmbqWU8jKauJVSysto4lZKKS+jiVsppbyMJm6llPIymriVUsrLaOJWSikvo4lbKaW8jCZupZTyMpq4lVLKy2jiVkopL6OJWymlvIwmbqWU8jKauJVSysto4lZKKS+jiVsppbyMJm6llPIymriVUsrLaOJWSikvo4lbKaW8jCZupZTyMpq4lVLKy2jiVkopL6OJWymlvMwZE7cxpqkxZokxJs4Ys8kYc39lBKaUUqp0vi7Mkw88LCJ/GGOCgdXGmB9EZHMFx6aUUqoUZ6xxi8hBEfnD+TgNiAMaV3RgSimlSndWbdzGmDCgA7CilPfuNMasMsasSkpKKp/olFJKncLlxG2MqQnMBR4QkdST3xeRySISKyKx9evXL88YlVJKFeNS4jbG+GGT9nQRmVexISmllDodV3qVGOADIE5EXqv4kJRSSp2OKzXubsBwoJcxZq1z+nMFx6WUUqoMZ+wOKCI/A6YSYlFKKa+Rk59DRl4GWXlZbEraxG97f+NY9jHeuOaNCl+2K/24lVJKFTNj4wxGfT6K7PzsE68ZDB0v7ohDHFQzFXtRuiZupZQ6C1PWTOGOBXdwRdMruKntTQT4BtCybks6N+5MSEBIpcSgiVsppVw0c+NMRi8YTd8Wffnsls+o7lfdLXHoIFNKqfMiIuw8upP5W+aTnJns7nAqTE5+Do/+8CixjWJZMHSB25I2aI1bKXUWRITtR7ezbPcy1h1ex5bkLaw/vJ7DGYcBCPAJ4Lb2t/FUj6cIrxPu5mjL1+TVk9mbupcPB31IgG+AW2PRxK2UKlNyZjKfb/mchfEL2XVsFzuP7uRI1hEAavrXpHW91lzT8houb3I5EfUimLlxJlPXTWXF/hWsHbsW32oXRorJzMvkhWUv0LN5T3qH93Z3OJq4VdW24fAGgvyCaF6rOX4+fu4OxyMcTj/MvLh5zImbw48JP1IgBTQObkyb+m0Y3GYwsY1i6dG8BxGhEdjr84pcGXYlfVr04cZZNzJp1ST+1vlvblqL85NXkMfCXQv5evvXBPkFcSDtAIczDjPn5jmnrLM7aOJWVVKBo4AHv3uQCb9PAMC3mi/3xN7DG9e84RE75vnKLcjl9vm34+/jz6t9X6Ve9XpnnH9J/BIm/zGZz7d8ToEUEBEawePdHmdI2yHEXBTjcrnc0PoGeoX34uklT/OXdn8htHpoeawSqTmp/O3rv5GYkcjwqOEMbjOYIL+g8/pOEeHj9R9zJPMITUKakJyZzI+7f2ThroUcyTpCdb/q5DvyyS3IZcClA+jerHu5rMv5MiJS7l8aGxsrq1atKvfvVao8pOakMmzeML7Y9gX3db6Pjhd3ZOGuhUzfMJ0Xer3A3//0d3eHWMLh9MP8uvdXVh1Yxbaj2/D38ae6b3W6NOnCgEsHEBoUytYjW0nNSaVrk64YDCM/H8m0ddPwreZL3aC6PH/V87Ss25L61evTKrQVgb6BHE4/zPQN0/ly25cs37ecrPws6lWvx6iYUYyIHkFk/chz/hHbmLiRmEkxDGo9iEERgwgNCqVfy34uNZ3EH4tn5YGVVPerTq2AWkTUiyC3IJdrP7mWTYmbaBzSmD0pe6gVUIuh7YYyKmYUnRt3PqdY31v9Hnd+eWeJ1xoFN6JXeC9uansT/Vr0w9/Hn+TMZGoH1q7QozJjzGoRiXVpXk3cqqpIzUnlrRVv8fry1zmefZwJ/SdwT6d7AFvzGvbZMD7Z8AnTB0/n1va3ujXWhOMJTFw5ka93fM3GxI0A+BgfWtRtgUMcHM8+TnJmMgaDbzVf8hx5ALSt35aYi2L4ZMMn/PPKfzKo9SBGzh/JmkNrTnx3NVON8NrhJBxPoEAKiG4YzVVhV9EzrCf9W/YvtxNvTy58kpd+eenE86HthvLxDR+fNnkvjl/M9TOuJy03rcTrPsaH6n7VmXPzHK6+5Gp+TPhJFzJbAAAW+UlEQVSRKWunMGfzHLLys4gIjWBE9AhujryZlnVbuhTfzqM7iZ4UTZcmXZg1ZBb70/ZT078m4bXD3XLUpYlbqZPkFuTS+b3OrDu8jgGXDuCZns8Q26jkPpKTn0Ofj/uw+uBqEu5PoH6Nih2eOLcgl4NpB6lXvR41/GsAsGLfCl797VXmxs2lmqnGlWFXcnX41fQM60l0w+gTTQMiwobEDSzYuoD03HSiGkaRV5DH68tfZ93hdYyOGc37A9/HGEO+I58tyVtIzkzmUPohNidtZnPSZlrUacHImJG0qd+mwtbxcPphMvIymLFxBuMXj+fGNjfy6Y2fnlJzPZ59nDmb53DPV/dwaeilfDDwA3yq+XAk8whbkrewN3UvI2NG0q5BuxKfS81JZfam2UxbP42fdv8EwKWhl3JF0ysI9g8mJCCEiNAIIhtEkp6bztbkrWTlZ9G8VnNe/vVlNiRuYMPdG2hWq1mFlYGrqkTidoiDQ+mHaBTcqEKXoy4ML/z0Ak8teYrZN81mSNshZc63JXkLbd9uy+PdHuffV/+73OPYkryF91a/x8xNM9mfth8Av2p+dG3SlXxHPr/t+41aAbW467K7uK/LfTQJaXJW3y8ibEraRJt6bfCp5lPu8Z+P1397nYe+f4jW9VozrvM42jVox2dbPuOr7V+x7cg2AHo078HnQz+ndmDts/7+hOMJfLH1C77a/hUbEjeQmZdJWk4aBVJQ5memXT+N4dHDz3mdytMFn7hFhNvn387/1v+Phy9/mH/1+pfb+1WqypdbkEtKdsoZa8bbjmwjamIUAyMGMuumWWf83qFzhvLV9q/Y/cBu6gbVPe84RYRF8Yv47y//5YddP+BbzZeBEQOJbhjNxTUvZsfRHSyMX0h2fjZjLxvLqA6jqOlf87yX64k+i/uMF5a9wOqDqwHw9/Gnd3hvujXtRqfGnbgq7KpybUfOd+Sz/ch2NiZuJDggmIjQCGr412BPyh6y8rLo3qy7x5yMvmASd4GjgP1p+2ka0rRE4f590d/598//plvTbvyy9xfaN2jPlEFTuKzRZee9TOV++Y58tiZv5XDGYRoHNybIL4i5m+fyycZPOJJ5hEDfQDLzMtmbuheHOIhqGMWQNkPwqebDtiPbqBtUl3s73UuLui3YeXQnt8+/nY2JG9nyty1cVPOiMy5/w+ENRE2K4ukeT/PcVc+dVezzt8znrRVvkZqTSmZeJhl5GaTlpHEs+xgX1byIcZ3HMbrDaBrWbHiuxeP1RITf9v3GgbQD9G3Rt9LG9/B0Xp+4HeJgXtw8nl7yNHHJcfRo3oN/9PgHBsMX277gzRVvcmfHO5k0YBLf7PiGOxbcQWJGIvd1vo/nr3qe4IDgclwbVVnyHfmM/nw0szfPLjHqWqFOjTrRul5rsvKz8Pfxp0WdFtTwq8GCbQv4de+vgO0RkJSRRL4jn7b127IpaRMGw9Trp57VIfHgmYNZHL+YBX9ZQPdm3c842lt2fjaPfP8Ib698m0tDL6VV3VZU96t+YoptFMtt7W/TI0NVJq9O3CLC8M+GM33DdFrXa82QNkP4YM0HHEw/CNihE4e2G8q0G6adODt9PPs44xeNZ+KqidQOrM09ne5hUMQg1h9ez+6U3dx12V00DtEb05clNSeVvSl7uTT0UrdehPK3r//G2yvf5q8d/0qP5j1oFNyIA2kHOJJ5hL4t+p72JFpSRhJBfkHU9K/JgbQD/N/v/8eyPcu4ttW13Nb+NprWanpWsWw4vIHuU7qTmpNK05Cm3HXZXdzb+V5qB9YmOTOZn3b/xKbETcQlx7E5aTNbkreQU5DDw5c/zIu9X8Tfx/98i0NVMV6duKetm8bt82/nye5P8vxVz+NTzYesvCzmxc0jtHoolze5nFqBtUr97KoDq3hx2YvM3zIfoWi9wmqHsXD4QlrUbXFOMV2IHOLgnZXvMG3dNFYfXI1DHAT6BhLbKJZ7O93LLZG3nGie2puyl8Xxi1l9cDXtG7SnT4s+hNUOK9d4JqyYwLhvx/HI5Y/wct+Xy/W7z1V6bjoLti5g6rqpfL/ze0ICQmhdrzUr9688sX2F1Q6jbf22tKnXhmtbXctV4Ve5OWrlrbw2cccfiyd6UjQxF8Ww5PYl53xWfGvyVtYeWkuHizuQkp1C/+n98fPxY8aNM+jRvIfLJyO2HdnGsHnD6B3em3/0/IdbRwM7X7kFufyw8weahDQhOCCYsV+O5YddP9C5cWf6tehHq7qtWHtoLd/u/JbNSZvp2qQr0Q2jWRy/mO1HtwP2RFJuQS4ALeq0oM8lfegZ1pOYi2JoXqs5m5M2s/7wejo17nRKt63iCrc5Ywybkzbz3I/PMWvTLAZFDGLuzXM9rjcEwNpDa/nPL/9h9/Hd9G3Rl2taXkP7Bu1PdONT6nx5ZeLOd+TT86OebEzcyPqx62leu3m5xROXFEff//VlX+o+Ol7ckad7PM2g1oNO+5mNiRu5etrVZORlkJ6bTljtMD4a9BE9w3qWW1yVqbAZolCQbxCv93udOy+7s8QPWYGjgGnrpvHUkqdIy0mjZ5gdVKdXeC/aNWjHluQtLNy1kIW7FrIkYQnpuemnLKuwOeuBrg/QrkG7Ej94DnFw0+ybmBc3D99qvuQ78qnpX5P7u9zP3//0d6/+cVTqfHhl4k7JTuHWebdya7tbuS3qtnKPKT03nf+t/x9vLH+DHUd3sHbs2jJrhVuTt9Ltw24E+AawaMQiEjMSGfX5KPIK8oi/P94ja4Sns2DrAgbNGMRdl91Fr/Be7D6+mwGXDjhtm7FDHIjIadc1ryCPTUmbWH94PfHH4mldrzWRDSKZvn46b/3+Fpl5mRgM7Rq0Y8aQGbSt35aJKydyz9f3MCpmFBfXvJiQgBDu6HjHGcfSUOpC55WJG+whdEX3qTySeYSI/4ugbf22/Djyx1KXN2TWEH7Y9QOr71x94vLZeXHzuHHWjXw+9HMGRgys0BjL0/7U/URPiqZZrWb8dsdvldarITEjkZ92/8TmpM1MXDUREWHKoCncNPsmrmh6Bd8N+85j+s8q5QnOJnF71B1wKmNHDq0eyktXv8SyPcv4eP3Hp7y/MXEjc+PmMq7zuBJjHlx36XVcXPNiJq2aVOExlofl+5Yzcv5I2rzdhqz8LD698dNK7YrWoEYDhrQdwtM9n2bxiMUA/PmTP1PNVDtxKbZS6tx4VOKuLKM7jKZrk648+sOjbErcVOK95396npr+NXnw8gdLvO7n48dfO/6Vb3d8S/yxeJeXlZqTSoGj7Etuy3Ik8wjvrnqXXlN78fB3D5/VZ9cdWkePKT34bMtn3Bx5M8tGLSOiXsRZx1Be2tRvw+LbF9OuQTveufYdjxgXQilvViUTdzVTjXcHvEu+I5+Yd2N49PtHWXtoLb/t/Y3Zm2ZzX+f7Sr3UeUzHMRhjmLx68hmXkZOfw4vLXuSiVy6iz8d9yMjNAODr7V/T4d0OtHirBeFvhvPAtw9wLOvYic8dzTrKYz88RuPXGjP2q7HEJcfx2vLX+HDNhy6tW25BLiPmj6BuUF12jtvJ+wPfp+PFHV0smYrTtn5bNty9gWFRw9wdilJez6PauCtbcmYyTy58kvfXvH/itRp+NUh4IKHMk2XXz7ieX/b+wvI7lpfaLzyvII9PN37K8z89z46jO7gy7Ep+2v0T3Zt1Z+ClA3ls4WNEhEbQ8eKOZORl8PmWz6kbVJfrIq5jf+p+ft//ux0vOmoYD13+EO0btOea6dfw856fWX7HcqIvij5lmYfSD7Fi3wqiGkbx4ZoP+deyf3ldW7xSVZ3Xnpx0ly3JW9iYuJH9qfuJbBDJ1ZdcXea8aw6u4eqP7ftzbppT4oKLn3b/xLB5w9ibupf2Ddrzcp+X6deyHzM3zuS2ebdRIAUMjBjIJ4M/OdH/d+2htTz8/cNsTtpMs1rNiAiN4JErHiGqYdSJ703MSKTDux0I8g1i+ZjlJX5UsvOz6fp+V9YdXnfitdujb+ej6z8qr+JRSlUCTdwVbOfRnVz36XVsO7KN74Z9R+9LeiMiRE2KIi0njXeufYf+LfuXOAH3zfZv2JS0iQe7PnhO3Ql/2/sbvab1IuaiGBaNWHSiv/PdX97NpNWTmHStPWm6O2U3j3d7vMyrS5VSnkkTdyVIzUml47sdCfQNZO3YtXy741uu+/Q6pl4/lRHRIypkmZ/FfcaQ2UPo37I/93S6h42JG3l84eM8dsVj/KfPfypkmUqpyqGJu5LM3TyXIbOHMHnAZKaum8q+1H1sv297hQ7UVHgBS6HuzbqzeMRivUO5Ul7ubBK33uX9PAxuM5juzbrz0PcPkZ6bzoT+Eyo8gd7d6W56hffiePZxfKr5ENUwSpO2UlWMJu7zYIzh1b6v0uX9LtSvXp/RHUZXynLd2SdbKeV+mrjPU+fGnXm5z8uE1w7XAZKUUpVCE3c5eOSKR9wdglKq0mUC2UAekO+cBAir8CVr4lZVmANIwu5sfkBtwLtGfjy9fOffs9nNc4H0UqZMoMA5OZxTWY8dQDPgCqC87yeZ51xWAFBe493ISd/lcC4nt9jfHGAt8APwO7ALOFLKd10EHCynuMrmwYnbAaRiN5paQOFdr/OAA8AWbAHVBeo7pwbYDUUHMPJeOUAKNukUUFSTOflxYS3HAVQHamB3wDzsZl3b+Vo6cAxYAywHEoCjQCKwzzl/oUDgUiAG6Af0BUJxbXtyYGtfmUBWGX/Leq+acx0MkIbd7lNPepxFUeIsTJIBzs/5YJOLYPeVGsBuYIdz3nrO9fBzlo2fc8pxlk8GRQm6eHmcr2pAuHOZvkBToAU2udUC/J3rVbx89mP/R6nO9fJ1/jXAIey+73C+FuycahZ7XPi8sDyrOafSHmdit4t12P9d4Y924Q9eaaoDXYGbgObO54Xl6ktRnqpYLiVuY8w1wJvYNXtfRF4q/1BygTeA1cAf2F80R7H3fSn6NT8dP2wSD8FumHlAY+wG1BC7wTQGLgciKX24lp3Az9gNfzcQhP1RiAKuxf6zPF0BdsdPw67DGmA7dn19sbWFg9ikloUtJ/8ypmrOeXKw5RuETXLF//o7vyMHmzjqYv8Hfs7n4dhaWDy2xpJYLM5j2JrvVmzZn+l/fK6CsYm5LnCJM54mFCW+vUAc8BUwzfkZP+w2Uzj5cWqyycLu+Ofi5O3aF1tuIc54Q7Dbc5AzzsKpGrasMyhK4mB/9JKB1sD12P/LYeyPVeGPXZ5zCsH+X2q6MAU5YytMfj5lPC5MjFuAZcA27A9KLjYh/4pNyiczzmU0wjY1NKXoR6rwB7otNlkGUbRtp5/0OJGiIwRxToXle/JjP+w+fZezLAqbOgKc7xVu/4U/dC2xecP9N3w+Yz9uY4wPtvT7YKsoK4G/iMjmsj5zbv24BbuBBgMdsRteKHajScXu3NWwiaKB8/3GFO30hVOi82+ac95qzrDji71eqCZ2Q6mPTca+2OS2w/m+j3MZ2didwYFNQn2wNYfmQBdnvOnAd8AqbFIsnI5id/iWFNWEDlK0oxbumNWxG05hjfN4sSkY+yMTCbRzrnuQs8wSsIdwB7AbWQG2BrGGU5NJsLM88oA6znWv6/wuP4oOCwunHOdfh3OeAOc82RQlq8IklkvRhp6B3XFOJ7BYGdd1Tq2ANtgf2MIajE+xxyc/98Pu8FnY/2u1YuuR4oyjhrOM2zvLzZWmkALs/3EZ9n+Y4pxSnesZhP1/nfy3tNfONI8vRUcKBc5yudCPGHOx5ZlLUTmUZ9OHdyrvftydgR0issv55TOAQUCZifvcGGwSquhDjQJsEv8Vu3Mexib7NOd7EcD9QG9ssi3sI52P3ZFnAouBb7CJDWfMhYeyARQdmoZiawlHgZ+wyawZtjbhh91hU7A/FNkU/bLXds4XhU36R4FNwJJiyyzOYH/M8pzfGQnc7VxOMPbQtAM2UVfWzpGNLdM8599d2P9vc6AT9sfKU/lgf5C7VNLyDPYHr6rwx7P//57PlcTdGHsMWWgfFbZFV0b7kA82IbcEzubSdF/gKucENkEexDap/ISt1Q3AFk1FneAqwDYlbKOoLbIRthbuaTetDaSoVg32B1EpVR5cSdylVdFOaV8xxtwJ3AnQrFlVGCjfYJPmzc6pMvhg22gvraTlKaU8kSs3UtiHPeYu1ATboFqCiEwWkVgRia1fXw+DlFKqoriSuFcCrYwx4cYYf2AosKBiw1JKKVWWMzaViEi+MeZv2C4TPsCHIrLpDB9TSilVQVzqxy0iXwNfV3AsSimlXFAlbxaslFLeTBO3Ukp5GU3cSinlZTRxK6WUl6mQe04aY5Kwg3Kci3rYgUG8jcZduTTuyqVxV7zmIuLSRTAVkrjPhzFmlasDrXgSjbtyadyVS+P2LNpUopRSXkYTt1JKeRlPTNyT3R3AOdK4K5fGXbk0bg/icW3cSimlTs8Ta9xKKaVOw2MStzHmGmPMVmPMDmPME+6OpyzGmKbGmCXGmDhjzCZjzP3O1+saY34wxmx3/q3j7lhLY4zxMcasMcZ86XweboxZ4Yx7pnMESI9jjKltjJljjNniLPvLvaHMjTEPOreTjcaYT40xgZ5Y5saYD40xicaYjcVeK7V8jfWWc19db4zp6GFxv+zcTtYbYz4zxtQu9t6Tzri3GmP6uSfq8+cRidt5X8u3gf7Ye339xRjT1r1RlSkfeFhE2mBv93yvM9YngEUi0gpY5Hzuie7H3hG30H+A151xHwPucEtUZ/Ym8K2ItAaisevg0WVujGkMjANiRaQddnTNoXhmmX8EXHPSa2WVb3/sDUJbYW+eMrGSYizNR5wa9w9AOxGJwt4u6kkA5346FHtvv2uAd5y5x+t4ROKm2H0tRSQXKLyvpccRkYMi8ofzcRo2gTTGxjvVOdtU7C22PYoxpgn2NvXvO58boBcwxzmLp8YdAvQAPgAQkVwROY4XlDl2BM4gY4wv9s64B/HAMheRn7A3Ny2urPIdBEwTazlQ2xhzceVEWlJpcYvI9yKS73y6HHvzF7BxzxCRHBGJx97stXOlBVuOPCVxl3Zfy8ZuisVlxpgw7F14VwANReQg2OSOvXuvp3kDeIyiW8yHAseLbeSeWu6XAEnAFGczz/vGmBp4eJmLyH7gFWAPNmGnAKvxjjKHssvXm/bX0dg7e4N3xX1anpK4XbqvpScxxtQE5gIPiEiqu+M5E2PMACBRRFYXf7mUWT2x3H2BjsBEEekAZOBhzSKlcbYJDwLCsTcorYFtZjiZJ5b56XjFdmOMGY9t2pxe+FIps3lc3K7wlMTt0n0tPYUxxg+btKeLyDzny4cLDxedfxPdFV8ZugEDjTEJ2KaoXtgaeG3nYTx4brnvA/aJyArn8znYRO7pZX41EC8iSSKSB8wDrsA7yhzKLl+P31+NMbcDA4DbpKjPs8fH7SpPSdxec19LZ7vwB0CciLxW7K0FwO3Ox7cDn1d2bKcjIk+KSBMRCcOW72IRuQ1YAgxxzuZxcQOIyCFgrzEmwvlSb2AzHl7m2CaSrsaY6s7tpjBujy9zp7LKdwEwwtm7pCuQUtik4gmMMdcAjwMDRSSz2FsLgKHGmABjTDj25Orv7ojxvImIR0zAn7FngHcC490dz2ni7I49vFoPrHVOf8a2Fy8Ctjv/1nV3rKdZhyuBL52PL8FuvDuA2UCAu+MrI+YYYJWz3OcDdbyhzIHngC3ARuBjIMATyxz4FNsOn4etmd5RVvlimxzedu6rG7C9Zjwp7h3YtuzC/XNSsfnHO+PeCvR3d7mf66RXTiqllJfxlKYSpZRSLtLErZRSXkYTt1JKeRlN3Eop5WU0cSullJfRxK2UUl5GE7dSSnkZTdxKKeVl/h8b1IKGxy4NpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    visualize_GOOGL()\n",
    "    LSTM_utilizando_GOOGL_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
